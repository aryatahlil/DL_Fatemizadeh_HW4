{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2su883KVVna_",
        "outputId": "016d8529-0824-4e2a-fe47-f2e46815fc05"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.tokenize import sent_tokenize\n",
        "# # for persian\n",
        "# !pip install hazm -q\n",
        "# from hazm import *\n",
        "import string\n",
        "from collections import Counter\n",
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BiQUny2-ZPwc",
        "outputId": "8aec8ca0-0192-47be-9a5d-4b502c09b9c1"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.tokenize import sent_tokenize\n",
        "import string\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "import pandas as pd\n",
        "class dataframe_generator:\n",
        "    def __init__(self, path):\n",
        "        seperator = ' [SEP] '\n",
        "        end = ' [END] '\n",
        "        self.lines = self.file_reader(path)\n",
        "        self.poems = []\n",
        "        for line,beyt in enumerate(self.lines):\n",
        "          if line >= 4:\n",
        "              if line % 2 == 0:\n",
        "                  Beyt = beyt[:-1] + seperator\n",
        "              elif line % 2 == 1:\n",
        "                  self.poems.append(Beyt + beyt[:-1] + end)\n",
        "        self.df = pd.DataFrame(self.poems)\n",
        "    def file_reader(self, path):\n",
        "        with open(path, 'r', encoding='utf-8') as f:\n",
        "            for row in f:\n",
        "              yield row\n",
        "\n",
        "class Fersdusi_Dataset(Dataset):\n",
        "    def __init__(self, poem_path):\n",
        "        self.poem_path = poem_path\n",
        "        self.word2idx = {}\n",
        "        self.idx2word = {}\n",
        "        self.idx = 0\n",
        "        self.create_dataset()\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.poem)-1\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.poem[idx], self.poem[idx+1] #each beyt is given and we want next beyt!\n",
        "\n",
        "    def create_dataset(self):\n",
        "        self.poem = dataframe_generator(self.poem_path).poems\n",
        "        self.poem = [word_tokenize(line) for line in self.poem]\n",
        "        self.poem = [self.clean_punctuatuin(line) for line in self.poem]\n",
        "        self.max_len = max([len(line) for line in self.poem])\n",
        "        self.poem = [self.make_same_length(line) for line in self.poem]\n",
        "        self.poem = [self.add_end_and_start_token(line) for line in self.poem]\n",
        "        self.create_vocab()\n",
        "        self.poem = [[self.word2idx[word] for word in line] for line in self.poem]\n",
        "        self.poem = torch.tensor(self.poem)\n",
        "        \n",
        "    def clean_punctuatuin(self,beyt):\n",
        "        punctuations = string.punctuation + \"<?><>.،؟،؛ََِِۀـّإـ,ـ«,ي\"\n",
        "        return [word for word in beyt if word not in punctuations]\n",
        "    def make_same_length(self,beyt):\n",
        "        return beyt + ['<pad>']*(self.max_len-len(beyt))\n",
        "    def add_end_and_start_token(self,beyt):\n",
        "        return ['<sos>'] + beyt + ['<eos>']\n",
        "    def create_vocab(self):\n",
        "        for beyt in self.poem:\n",
        "            for word in beyt:\n",
        "                self.addword(word)\n",
        "        pass\n",
        "    def addword(self,word):\n",
        "        if word not in self.word2idx:\n",
        "            self.word2idx[word] = self.idx\n",
        "            self.idx2word[self.idx] = word\n",
        "            self.idx += 1\n",
        "        pass\n",
        "\n",
        "\n",
        "\n",
        "        \n",
        "        \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6le7s42EZVJn",
        "outputId": "f462cbf7-aa5b-4f3d-d7cb-b06e07d81a17"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of data in train: 39684\n",
            "Number of data in test: 9922\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "dataset = Fersdusi_Dataset('ferdousi.txt')\n",
        "# split dataset with torch\n",
        "train_dataset, test_dataset = train_test_split(dataset, test_size=0.2)\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=True)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f'Number of data in train: {len(train_dataset)}')\n",
        "print(f'Number of data in test: {len(test_dataset)}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9eNrvvhwEefh"
      },
      "outputs": [],
      "source": [
        "### source of models is pytorch github ###\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, input_size, embedding_size, hidden_size, num_layers, p_drop=0.2):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "        self.embedding = nn.Embedding(input_size, embedding_size)\n",
        "        self.rnn = nn.LSTM(embedding_size, hidden_size, num_layers, batch_first=True)\n",
        "        self.dropout = nn.Dropout(p_drop)\n",
        "\n",
        "    def forward(self, x, hidden=None, cell=None):\n",
        "        embedding = self.dropout(self.embedding(x))\n",
        "        if hidden is None and cell is None:\n",
        "            hidden = torch.zeros((self.num_layers, x.shape[0], self.hidden_size), device=x.device)\n",
        "            cell = torch.zeros((self.num_layers, x.shape[0], self.hidden_size), device=x.device)\n",
        "        outputs, (hidden, cell) = self.rnn(embedding, (hidden, cell))\n",
        "        return hidden, cell\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, input_size,encoder_emebding ,embedding_size, hidden_size, output_size, num_layers, p_drop=0.2):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "        self.embedding = encoder_emebding\n",
        "        self.rnn = nn.LSTM(embedding_size, hidden_size, num_layers, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, output_size)\n",
        "        self.dropout = nn.Dropout(p_drop)\n",
        "\n",
        "    def forward(self, x, hidden, cell):\n",
        "        x = x.unsqueeze(1)\n",
        "        embedding = self.dropout(self.embedding(x))\n",
        "        outputs, (hidden, cell) = self.rnn(embedding, (hidden, cell))\n",
        "        predictions = self.fc(outputs)\n",
        "        predictions = predictions.squeeze(1)\n",
        "        return predictions, hidden, cell\n",
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self, encoder, decoder):\n",
        "        super(Seq2Seq, self).__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "    def forward(self, source, target, teacher_forcing_ratio=0.5):\n",
        "        batch_size = source.shape[0]\n",
        "        target_len = target.shape[1]\n",
        "        target_vocab_size = len(dataset.word2idx)\n",
        "        outputs = torch.zeros(batch_size, target_len, target_vocab_size).to(device)\n",
        "        hidden, cell = self.encoder(source)\n",
        "        x = target[:, 0]\n",
        "        for t in range(1, target_len):\n",
        "            output, hidden, cell = self.decoder(x, hidden, cell)\n",
        "            outputs[:, t] = output\n",
        "            best_guess = output.argmax(1)\n",
        "            x = target[:, t] if random.random() < teacher_forcing_ratio else best_guess\n",
        "        return outputs\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class EncoderGRU(nn.Module):\n",
        "    def __init__(self, input_size, embedding_size, hidden_size, num_layers, p_drop=0.2):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "        self.embedding = nn.Embedding(input_size, embedding_size)\n",
        "        self.rnn = nn.GRU(embedding_size, hidden_size, num_layers, batch_first=True, bidirectional=True)\n",
        "        self.dropout = nn.Dropout(p_drop)\n",
        "\n",
        "    def forward(self, x, hidden=None, cell=None):\n",
        "        # x.shape = (batch_size, seq_len)\n",
        "        embedding = self.dropout(self.embedding(x))\n",
        "        # embedding.shape = (batch_size, seq_len, embedding_size)\n",
        "        if hidden is None and cell is None:\n",
        "            hidden = torch.zeros((self.num_layers, x.shape[0], self.hidden_size), device=x.device)\n",
        "            cell = torch.zeros((self.num_layers, x.shape[0], self.hidden_size), device=x.device)\n",
        "        outputs, (hidden, cell) = self.rnn(embedding, (hidden, cell))\n",
        "\n",
        "        return hidden, cell\n",
        "class DecoderGRU(nn.Module):\n",
        "    def __init__(self, input_size,encoder_emebding ,embedding_size, hidden_size, output_size, num_layers, p_drop=0.2):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "        self.embedding = encoder_emebding\n",
        "        self.rnn = nn.GRU(embedding_size, hidden_size, num_layers, batch_first=True, bidirectional=True)\n",
        "        self.fc = nn.Linear(hidden_size, output_size)\n",
        "        self.dropout = nn.Dropout(p_drop)\n",
        "\n",
        "    def forward(self, x, hidden, cell):\n",
        "        x = x.unsqueeze(1)\n",
        "        embedding = self.dropout(self.embedding(x))\n",
        "        outputs, (hidden, cell) = self.rnn(embedding, (hidden, cell))\n",
        "        predictions = self.fc(outputs)\n",
        "        predictions = predictions.squeeze(1)\n",
        "        return predictions, hidden, cell\n",
        "class Seq2SeqGRU(nn.Module):\n",
        "    def __init__(self, encoder, decoder):\n",
        "        super(Seq2Seq, self).__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "    def forward(self, source, target, teacher_forcing_ratio=0.5):\n",
        "        batch_size = source.shape[0]\n",
        "        target_len = target.shape[1]\n",
        "        target_vocab_size = len(dataset.word2idx)\n",
        "        outputs = torch.zeros(batch_size, target_len, target_vocab_size).to(device)\n",
        "        hidden, cell = self.encoder(source)\n",
        "        x = target[:, 0]\n",
        "        for t in range(1, target_len):\n",
        "            output, hidden, cell = self.decoder(x, hidden, cell)\n",
        "            outputs[:, t] = output\n",
        "            best_guess = output.argmax(1)\n",
        "            x = target[:, t] if random.random() < teacher_forcing_ratio else best_guess\n",
        "        return outputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gmdu4iBQMYWy"
      },
      "outputs": [],
      "source": [
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "def convert_to_sentence(output):\n",
        "    sentence = []\n",
        "    for idx in output:\n",
        "        sentence.append(dataset.idx2word[idx] if idx != dataset.word2idx['<pad>'] else '')\n",
        "    return sentence\n",
        "def generate_sentence(model, source, max_len=50):\n",
        "    source = source.unsqueeze(0)\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        hidden, cell = model.encoder(source.to(device))\n",
        "        x = torch.tensor([dataset.word2idx['<sos>']]).to(device)\n",
        "        outputs = []\n",
        "        for t in range(max_len):\n",
        "            output, hidden, cell = model.decoder(x, hidden, cell)\n",
        "            # output.shape = (1, output_size)\n",
        "            best_guess = output.argmax(1)\n",
        "            outputs.append(best_guess.item())\n",
        "            x = best_guess\n",
        "            if best_guess.item() == dataset.word2idx['<eos>']:\n",
        "                break\n",
        "        return convert_to_sentence(outputs)\n",
        "\n",
        "def check_generated_verse(sentence):\n",
        "    is_good = True\n",
        "    repated_treshhold = 2\n",
        "    repated = 0\n",
        "    for i in range(len(sentence)-1):\n",
        "        if sentence[i] == sentence[i+1]:\n",
        "            repated += 1\n",
        "            if repated > repated_treshhold:\n",
        "                is_good = False\n",
        "                return is_good\n",
        "    if len(set(sentence)) < 3:\n",
        "        is_good = False\n",
        "        return is_good\n",
        "    if len(sentence) < 25:\n",
        "        is_good = False\n",
        "        return is_good\n",
        "    return is_good\n",
        "def generate_good_sentence(model, source, max_len=50):\n",
        "    for i in range(10):\n",
        "        sentence = generate_sentence(model, source, max_len)\n",
        "        if check_generated_verse(sentence):\n",
        "            sentence = ' '.join(sentence)\n",
        "            return sentence\n",
        "    model.train()\n",
        "    sentence = [word for word in sentence if word not in ['<eos>', '<pad>', '<sos>', '<unk>', 'SEP','END']]\n",
        "    sentence = ' '.join(sentence)\n",
        "    return sentence\n",
        "def evalute_model_by_generate_sentence(model, num_of_sentences=3):\n",
        "  ''' I USE this function to recheck generated sentence to be good . because the model is not complicated enough and our resources is limited '''\n",
        "    for i in range(num_of_sentences):\n",
        "        idx = random.randint(0, len(test_dataset))\n",
        "        verse = test_dataset[idx][0].to(device)\n",
        "    verse_generated = generate_good_sentence(model, verse)\n",
        "    model.train()\n",
        "    return verse_generated\n",
        "    \n",
        "def train(model, iterator, optimizer, criterion, clip, num_epochs,print_every=100, teacher_forcing_ratio=0.5,num_of_sentences=1):\n",
        "    train_loss_list = []\n",
        "    model.train()\n",
        "    print(f\"\\n Generated before training {evalute_model_by_generate_sentence(model, num_of_sentences)}:\\n\")\n",
        "    \n",
        "    for epoch in range(num_epochs):\n",
        "        epoch_loss = 0\n",
        "        for i, (source, target) in enumerate(iterator):\n",
        "            source = source.to(device)\n",
        "            target = target.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            output = model(source, target, teacher_forcing_ratio)\n",
        "            # output.shape = (batch_size, target_len, output_dim)\n",
        "            output_dim = output.shape[-1]\n",
        "            output = output[1:].view(-1, output_dim)\n",
        "            target = target[1:].view(-1)\n",
        "            loss = criterion(output, target)\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "            optimizer.step()\n",
        "            epoch_loss += loss.item()\n",
        "            if i % print_every == 0:\n",
        "                print(f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(iterator)}], Loss: {loss.item():.4f}')\n",
        "        train_loss_list.append(epoch_loss/len(iterator))\n",
        "        print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {epoch_loss/len(iterator):.4f}')\n",
        "    print(f\"\\n Generated After training : {evalute_model_by_generate_sentence(model, num_of_sentences)} \\n\")\n",
        "    return model, train_loss_list\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "###### Train WITH LSTM MODEL ####\n",
        "input_size_decoder = len(dataset.word2idx)\n",
        "input_size_encoder = len(dataset.word2idx)\n",
        "output_size = len(dataset.word2idx)\n",
        "decoder_embedding_size = 50\n",
        "encoder_embedding_size = 50\n",
        "hidden_size = 100\n",
        "num_layers = 1\n",
        "num_epochs = 20\n",
        "clip = 1\n",
        "\n",
        "# create model\n",
        "encoder_net = Encoder(input_size_encoder, encoder_embedding_size, hidden_size, num_layers).to(device)\n",
        "decoder_net = Decoder(input_size_decoder,encoder_net.embedding, decoder_embedding_size, hidden_size, output_size, num_layers).to(device)\n",
        "model_LSTM = Seq2Seq(encoder_net, decoder_net).to(device)\n",
        "### train the model ### \n",
        "optimizer = optim.Adam(model_LSTM.parameters())\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=dataset.word2idx['<pad>'])\n",
        "model_LSTM_trained, train_losses_LSTM = train(model_LSTM, train_loader, optimizer, criterion, clip, num_epochs,print_every=500, teacher_forcing_ratio=0.2,num_of_sentences=1)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c5J15IYNpPPO",
        "outputId": "6f12e080-b2ba-46a4-be9f-bccd9add64fd"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Generated before training خوانی سرگذشت افسران کافرین بابکان مانوی درتاج نامورتان چوخستو داست بازگشت بیامدش بدنهاد سربریدن بیامدش سربریدن فسونی چنانند ببندد قیصرکشان ناگه ورشک طلایه بگذشته اشتر بهارم کنارنگی نیستتان چیم بشنو رشک بشنو بشنو بترسیدی تپد بکوب شراع بینوا بینوا بربر اندرآیی هرومش کردن باید اورمزدت بخوایید سمرقند انجا جوشنوران خویشت:\n",
            "\n",
            "Epoch [1/20], Step [1/1241], Loss: 9.8047\n",
            "Epoch [1/20], Step [501/1241], Loss: 6.0430\n",
            "Epoch [1/20], Step [1001/1241], Loss: 5.9056\n",
            "Epoch [1/20], Train Loss: 6.1615\n",
            "Epoch [2/20], Step [1/1241], Loss: 5.9005\n",
            "Epoch [2/20], Step [501/1241], Loss: 5.8881\n",
            "Epoch [2/20], Step [1001/1241], Loss: 5.9510\n",
            "Epoch [2/20], Train Loss: 5.9174\n",
            "Epoch [3/20], Step [1/1241], Loss: 5.7792\n",
            "Epoch [3/20], Step [501/1241], Loss: 5.6977\n",
            "Epoch [3/20], Step [1001/1241], Loss: 6.0628\n",
            "Epoch [3/20], Train Loss: 5.8700\n",
            "Epoch [4/20], Step [1/1241], Loss: 5.8786\n",
            "Epoch [4/20], Step [501/1241], Loss: 5.9798\n",
            "Epoch [4/20], Step [1001/1241], Loss: 5.7637\n",
            "Epoch [4/20], Train Loss: 5.8385\n",
            "Epoch [5/20], Step [1/1241], Loss: 5.7725\n",
            "Epoch [5/20], Step [501/1241], Loss: 5.7724\n",
            "Epoch [5/20], Step [1001/1241], Loss: 5.6670\n",
            "Epoch [5/20], Train Loss: 5.8147\n",
            "Epoch [6/20], Step [1/1241], Loss: 5.6746\n",
            "Epoch [6/20], Step [501/1241], Loss: 5.7338\n",
            "Epoch [6/20], Step [1001/1241], Loss: 5.8028\n",
            "Epoch [6/20], Train Loss: 5.7957\n",
            "Epoch [7/20], Step [1/1241], Loss: 5.6142\n",
            "Epoch [7/20], Step [501/1241], Loss: 5.5064\n",
            "Epoch [7/20], Step [1001/1241], Loss: 5.8380\n",
            "Epoch [7/20], Train Loss: 5.7793\n",
            "Epoch [8/20], Step [1/1241], Loss: 5.7300\n",
            "Epoch [8/20], Step [501/1241], Loss: 5.6639\n",
            "Epoch [8/20], Step [1001/1241], Loss: 5.8898\n",
            "Epoch [8/20], Train Loss: 5.7649\n",
            "Epoch [9/20], Step [1/1241], Loss: 5.8182\n",
            "Epoch [9/20], Step [501/1241], Loss: 5.7349\n",
            "Epoch [9/20], Step [1001/1241], Loss: 5.6744\n",
            "Epoch [9/20], Train Loss: 5.7499\n",
            "Epoch [10/20], Step [1/1241], Loss: 5.6786\n",
            "Epoch [10/20], Step [501/1241], Loss: 5.7801\n",
            "Epoch [10/20], Step [1001/1241], Loss: 5.6514\n",
            "Epoch [10/20], Train Loss: 5.7379\n",
            "Epoch [11/20], Step [1/1241], Loss: 5.8059\n",
            "Epoch [11/20], Step [501/1241], Loss: 5.7531\n",
            "Epoch [11/20], Step [1001/1241], Loss: 5.8595\n",
            "Epoch [11/20], Train Loss: 5.7257\n",
            "Epoch [12/20], Step [1/1241], Loss: 5.4540\n",
            "Epoch [12/20], Step [501/1241], Loss: 5.6580\n",
            "Epoch [12/20], Step [1001/1241], Loss: 5.5897\n",
            "Epoch [12/20], Train Loss: 5.7131\n",
            "Epoch [13/20], Step [1/1241], Loss: 5.7302\n",
            "Epoch [13/20], Step [501/1241], Loss: 5.5754\n",
            "Epoch [13/20], Step [1001/1241], Loss: 5.6048\n",
            "Epoch [13/20], Train Loss: 5.7040\n",
            "Epoch [14/20], Step [1/1241], Loss: 5.6354\n",
            "Epoch [14/20], Step [501/1241], Loss: 5.6106\n",
            "Epoch [14/20], Step [1001/1241], Loss: 5.7935\n",
            "Epoch [14/20], Train Loss: 5.6927\n",
            "Epoch [15/20], Step [1/1241], Loss: 5.6246\n",
            "Epoch [15/20], Step [501/1241], Loss: 5.7179\n",
            "Epoch [15/20], Step [1001/1241], Loss: 5.5710\n",
            "Epoch [15/20], Train Loss: 5.6785\n",
            "Epoch [16/20], Step [1/1241], Loss: 5.7161\n",
            "Epoch [16/20], Step [501/1241], Loss: 5.6966\n",
            "Epoch [16/20], Step [1001/1241], Loss: 5.6598\n",
            "Epoch [16/20], Train Loss: 5.6645\n",
            "Epoch [17/20], Step [1/1241], Loss: 5.6184\n",
            "Epoch [17/20], Step [501/1241], Loss: 5.7680\n",
            "Epoch [17/20], Step [1001/1241], Loss: 5.4657\n",
            "Epoch [17/20], Train Loss: 5.6458\n",
            "Epoch [18/20], Step [1/1241], Loss: 5.4990\n",
            "Epoch [18/20], Step [501/1241], Loss: 5.6915\n",
            "Epoch [18/20], Step [1001/1241], Loss: 5.7464\n",
            "Epoch [18/20], Train Loss: 5.6330\n",
            "Epoch [19/20], Step [1/1241], Loss: 5.5041\n",
            "Epoch [19/20], Step [501/1241], Loss: 5.4435\n",
            "Epoch [19/20], Step [1001/1241], Loss: 5.4789\n",
            "Epoch [19/20], Train Loss: 5.6166\n",
            "Epoch [20/20], Step [1/1241], Loss: 5.5944\n",
            "Epoch [20/20], Step [501/1241], Loss: 5.7542\n",
            "Epoch [20/20], Step [1001/1241], Loss: 5.5059\n",
            "Epoch [20/20], Train Loss: 5.6057\n",
            "\n",
            " Generated After training : چو گفت است و و رفت کاش شیر \n",
            " \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## create the model ##\n",
        "encoder_net_GRU = Encoder(input_size_encoder, encoder_embedding_size, hidden_size, num_layers).to(device)\n",
        "decoder_net_GRU = Decoder(input_size_decoder,encoder_net.embedding, decoder_embedding_size, hidden_size, output_size, num_layers).to(device)\n",
        "model_GRU = Seq2Seq(encoder_net_GRU, decoder_net_GRU).to(device)\n",
        "##### TRAIN WITH GRU #####\n",
        "optimizer = optim.Adam(model_GRU.parameters())\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=dataset.word2idx['<pad>'])\n",
        "num_epochs = 20\n",
        "clip = 1\n",
        "model_GRU , train_losses = train(model_GRU, train_loader, optimizer, criterion, clip, num_epochs,print_every=500, teacher_forcing_ratio=0.2,num_of_sentences=1)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hnQEVSLvpo7F",
        "outputId": "e588cb3e-adf6-4a33-b170-2e04bbb6c750"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Generated before training خسرو خسرو توآییم ناسپاسان فرونتر تختست بدکاستی ستیزه خدایا توخواهی کتان دربرتری عیار عیار بکوشم چوبالوی عیار عیار بکوشم چوبالوی عیار عیار بکوشم چوبالوی عیار عیار بکوشم چوبالوی عیار عیار بکوشم چوبالوی عیار عیار بکوشم چوبالوی عیار عیار بکوشم چوبالوی عیار عیار بکوشم چوبالوی عیار عیار بکوشم چوبالوی عیار عیار:\n",
            "\n",
            "Epoch [1/20], Step [1/1241], Loss: 9.8006\n",
            "Epoch [1/20], Step [501/1241], Loss: 6.1790\n",
            "Epoch [1/20], Step [1001/1241], Loss: 5.7982\n",
            "Epoch [1/20], Train Loss: 6.1307\n",
            "Epoch [2/20], Step [1/1241], Loss: 5.9662\n",
            "Epoch [2/20], Step [501/1241], Loss: 5.9200\n",
            "Epoch [2/20], Step [1001/1241], Loss: 5.9358\n",
            "Epoch [2/20], Train Loss: 5.8606\n",
            "Epoch [3/20], Step [1/1241], Loss: 5.8567\n",
            "Epoch [3/20], Step [501/1241], Loss: 5.8167\n",
            "Epoch [3/20], Step [1001/1241], Loss: 5.8273\n",
            "Epoch [3/20], Train Loss: 5.8197\n",
            "Epoch [4/20], Step [1/1241], Loss: 5.9774\n",
            "Epoch [4/20], Step [501/1241], Loss: 5.8160\n",
            "Epoch [4/20], Step [1001/1241], Loss: 5.6584\n",
            "Epoch [4/20], Train Loss: 5.7918\n",
            "Epoch [5/20], Step [1/1241], Loss: 5.7586\n",
            "Epoch [5/20], Step [501/1241], Loss: 5.7188\n",
            "Epoch [5/20], Step [1001/1241], Loss: 5.8359\n",
            "Epoch [5/20], Train Loss: 5.7668\n",
            "Epoch [6/20], Step [1/1241], Loss: 5.4709\n",
            "Epoch [6/20], Step [501/1241], Loss: 5.7632\n",
            "Epoch [6/20], Step [1001/1241], Loss: 5.6590\n",
            "Epoch [6/20], Train Loss: 5.7470\n",
            "Epoch [7/20], Step [1/1241], Loss: 5.7463\n",
            "Epoch [7/20], Step [501/1241], Loss: 5.8158\n",
            "Epoch [7/20], Step [1001/1241], Loss: 5.9063\n",
            "Epoch [7/20], Train Loss: 5.7351\n",
            "Epoch [8/20], Step [1/1241], Loss: 5.5745\n",
            "Epoch [8/20], Step [501/1241], Loss: 5.8567\n",
            "Epoch [8/20], Step [1001/1241], Loss: 5.6951\n",
            "Epoch [8/20], Train Loss: 5.7203\n",
            "Epoch [9/20], Step [1/1241], Loss: 5.7044\n",
            "Epoch [9/20], Step [501/1241], Loss: 5.5051\n",
            "Epoch [9/20], Step [1001/1241], Loss: 5.7348\n",
            "Epoch [9/20], Train Loss: 5.7067\n",
            "Epoch [10/20], Step [1/1241], Loss: 5.6380\n",
            "Epoch [10/20], Step [501/1241], Loss: 5.7309\n",
            "Epoch [10/20], Step [1001/1241], Loss: 5.5283\n",
            "Epoch [10/20], Train Loss: 5.6900\n",
            "Epoch [11/20], Step [1/1241], Loss: 5.5668\n",
            "Epoch [11/20], Step [501/1241], Loss: 5.2691\n",
            "Epoch [11/20], Step [1001/1241], Loss: 5.4163\n",
            "Epoch [11/20], Train Loss: 5.6789\n",
            "Epoch [12/20], Step [1/1241], Loss: 5.5357\n",
            "Epoch [12/20], Step [501/1241], Loss: 5.5128\n",
            "Epoch [12/20], Step [1001/1241], Loss: 5.5292\n",
            "Epoch [12/20], Train Loss: 5.6564\n",
            "Epoch [13/20], Step [1/1241], Loss: 5.5952\n",
            "Epoch [13/20], Step [501/1241], Loss: 5.5977\n",
            "Epoch [13/20], Step [1001/1241], Loss: 5.7945\n",
            "Epoch [13/20], Train Loss: 5.6450\n",
            "Epoch [14/20], Step [1/1241], Loss: 5.2464\n",
            "Epoch [14/20], Step [501/1241], Loss: 5.6697\n",
            "Epoch [14/20], Step [1001/1241], Loss: 5.9120\n",
            "Epoch [14/20], Train Loss: 5.6300\n",
            "Epoch [15/20], Step [1/1241], Loss: 5.4247\n",
            "Epoch [15/20], Step [501/1241], Loss: 5.5333\n",
            "Epoch [15/20], Step [1001/1241], Loss: 5.5386\n",
            "Epoch [15/20], Train Loss: 5.6148\n",
            "Epoch [16/20], Step [1/1241], Loss: 5.4257\n",
            "Epoch [16/20], Step [501/1241], Loss: 5.4022\n",
            "Epoch [16/20], Step [1001/1241], Loss: 5.6704\n",
            "Epoch [16/20], Train Loss: 5.6052\n",
            "Epoch [17/20], Step [1/1241], Loss: 5.2071\n",
            "Epoch [17/20], Step [501/1241], Loss: 5.3559\n",
            "Epoch [17/20], Step [1001/1241], Loss: 5.3192\n",
            "Epoch [17/20], Train Loss: 5.5937\n",
            "Epoch [18/20], Step [1/1241], Loss: 5.5747\n",
            "Epoch [18/20], Step [501/1241], Loss: 5.4286\n",
            "Epoch [18/20], Step [1001/1241], Loss: 5.6630\n",
            "Epoch [18/20], Train Loss: 5.5774\n",
            "Epoch [19/20], Step [1/1241], Loss: 5.4431\n",
            "Epoch [19/20], Step [501/1241], Loss: 5.5971\n",
            "Epoch [19/20], Step [1001/1241], Loss: 5.5285\n",
            "Epoch [19/20], Train Loss: 5.5673\n",
            "Epoch [20/20], Step [1/1241], Loss: 5.2092\n",
            "Epoch [20/20], Step [501/1241], Loss: 5.3410\n",
            "Epoch [20/20], Step [1001/1241], Loss: 5.4541\n",
            "Epoch [20/20], Train Loss: 5.5515\n",
            "\n",
            " Generated After training : با عبای بکوشم و با وقت رفت \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "7xe2s8WYihQ5",
        "outputId": "d8db418c-c6f4-4d18-867f-e32ef9d62e76"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "plt.plot(train_losses, label='GRU')\n",
        "plt.plot(train_losses_LSTM, label='LSTM')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g3TQZLnPilTA"
      },
      "source": [
        "**AS plot showing , GRU is working better in this task**"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}